services:
  php-backend:
    build:
      context: ../php-backend
    ports:
      - "8181:8181"
    env_file:
      - ../php-backend/.env
    command: php -d variables_order=EGPCS -d upload_max_filesize=100M -d post_max_size=100M -d memory_limit=128M -d max_execution_time=180 -d max_input_time=180 -S 0.0.0.0:8181 -t public public/router.php
    depends_on:
      - ai-service

  ai-service:
    build:
      context: ../ai-service
      dockerfile: Dockerfile
    image: studystreak-ai:latest
    ports:
      - "8000:8000"
    env_file:
      - ../ai-service/.env
    volumes:
      # Mount code for development (hot reload)
      - ../ai-service:/app
      # Persistent model cache (avoid re-downloading models)
      - ai-models-cache:/app/model_cache
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload
    # GPU options (choose one depending on your Docker version/runtime):
    # NOTE: some Compose/validation setups reject `device_requests` in the base
    # docker-compose.yml. If you see a validation error about `device_requests`
    # being an additional property, use the GPU override file included alongside
    # this compose file:
    #
    #   docker compose -f docker-compose.yml -f docker-compose.gpu.yml up -d --build ai-service
    #
    # The override file (docker-compose.gpu.yml) adds the `device_requests` block
    # only when you explicitly opt-in, avoiding validation errors on systems
    # that don't accept the property in the base file.
    # Alternative: run the built image manually with `--gpus all`:
    #    docker run --rm --gpus all -p 8000:8000 studystreak-ai:latest
    restart: unless-stopped

  context7-mcp:
    build:
      context: ./
    ports:
      - "8090:8080"

volumes:
  ai-models-cache:
