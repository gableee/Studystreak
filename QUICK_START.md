# ğŸš€ Quick Start Guide - StudyStreak AI with Ollama

## Start in 3 Steps

### 1ï¸âƒ£ Start Ollama
```powershell
# Verify Ollama is running
ollama list

# If needed, pull the model
ollama pull qwen3-vl:8b
```

### 2ï¸âƒ£ Start AI Service
```powershell
cd ai-service
python main.py

# Service runs on: http://localhost:8000
# API Docs: http://localhost:8000/docs
```

### 3ï¸âƒ£ Test It
```powershell
# Run tests
python test_ai_service.py

# Try example client
python example_client.py
```

---

## ğŸ“ API Usage

### From Python
```python
import requests

response = requests.post('http://localhost:8000/generate/studytools', json={
    'supabase_file_path': 'user123/notes.pdf',
    'num_quiz_questions': 5,
    'num_flashcards': 10
})

studytools = response.json()['studytools']
```

### From PHP
```php
$ch = curl_init('http://localhost:8000/generate/studytools');
curl_setopt($ch, CURLOPT_POST, true);
curl_setopt($ch, CURLOPT_POSTFIELDS, json_encode([
    'supabase_file_path' => 'user123/notes.pdf',
    'num_quiz_questions' => 5,
    'num_flashcards' => 10
]));
curl_setopt($ch, CURLOPT_HTTPHEADER, ['Content-Type: application/json']);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($ch);
$data = json_decode($response, true);
```

### From curl
```bash
curl -X POST http://localhost:8000/generate/studytools \
  -H "Content-Type: application/json" \
  -d '{"content": "Your text here", "num_quiz_questions": 5}'
```

---

## ğŸ“ File Structure

```
ai-service/
â”œâ”€â”€ main.py                    # FastAPI app (start here)
â”œâ”€â”€ config.py                  # Configuration
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ .env                       # Your environment variables
â”œâ”€â”€ start.ps1                  # Startup script
â”œâ”€â”€ test_ai_service.py        # Test suite
â”œâ”€â”€ example_client.py         # Usage examples
â”œâ”€â”€ extractors/
â”‚   â””â”€â”€ document_extractor.py # PDF/DOCX/PPTX/OCR extraction
â”œâ”€â”€ models/
â”‚   â””â”€â”€ studytools_generator.py # AI content generation
â”œâ”€â”€ routes/
â”‚   â””â”€â”€ generation.py         # API endpoints
â””â”€â”€ utils/
    â”œâ”€â”€ ollama_client.py      # Ollama API wrapper
    â””â”€â”€ supabase_client.py    # Supabase storage client
```

---

## ğŸ”§ Configuration (.env)

```bash
# Supabase (required for file fetching)
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_KEY=your-key-here

# Ollama (optional, defaults shown)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen3-vl:8b

# Service (optional)
PORT=8000
```

---

## ğŸ¯ Available Endpoints

| Endpoint | What It Does |
|----------|--------------|
| `POST /generate/studytools` | All: summary + keypoints + quiz + flashcards |
| `POST /generate/summary` | Summary only |
| `POST /generate/keypoints` | Keypoints only |
| `POST /generate/quiz` | Quiz only |
| `POST /generate/flashcards` | Flashcards only |
| `POST /generate/upload-and-generate` | Upload file directly |
| `GET /health` | Check service + Ollama status |

---

## ğŸ’¡ Tips

**Faster Generation:**
- Use smaller model: `ollama pull qwen3-vl:4b`
- Reduce question/card counts

**Better Quality:**
- Use larger model: `qwen3-vl:8b`
- Provide specific `assignment` descriptions

**Debugging:**
- Check logs: `ai-service/logs/ai-service.log`
- Use `/health` endpoint to verify Ollama
- Visit `/docs` for interactive API testing

---

## âš ï¸ Common Issues

**"Ollama not available"**
â†’ Start Ollama or check if it's on port 11434

**"Model not found"**
â†’ Run `ollama pull qwen3-vl:8b`

**"Empty JSON response"**
â†’ Known issue with vision models, fallback handling is implemented

**"Timeout"**
â†’ Increase timeout in config or use smaller model

---

## ğŸ“š Full Documentation

- Setup: `README_OLLAMA_INTEGRATION.md`
- Summary: `IMPLEMENTATION_SUMMARY.md`
- API Docs: http://localhost:8000/docs (when running)

---

**Ready to Go!** ğŸ‰

Start the service and begin generating educational content from your uploaded materials!
